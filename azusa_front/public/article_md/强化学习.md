# 1. 强化学习

强化学习(reinforcement learning，RL)讨论的问题是智能体`agent`怎么在复杂、不确定的环境`environment`里面去最大化它能获得的奖励。

在强化学习过程中，智能体与环境一直在交互。智能体在环境里面获取某个状态后，它会利用该状态输出一个动作`action`(决策`decision`)。然后这个动作会在环境之中被执行，环境会根据智能体采取的动作，输出下一个状态以及当前这个动作带来的奖励。智能体的目的就是尽可能多地从环境中获取奖励。

<div style="display: flex; justify-content: center; align-items: center;">
    <div style="text-align: center;">
        <img src="assets/强化学习/images/image.png" style="width: 50%;"/>
        <p style="font-size: small; color: gray;">强化学习示意图</p>
    </div>
</div>

## 1.1 为什么需要强化学习
监督学习要求数据满足$\text{i.i.d}$，而且需要有标签。而在游戏等领域，这两个假设都不成立，游戏的上一帧与下一帧之间的关系非常强(连续性)，奖励稀疏(无及时反馈/延迟奖励)，这就导致了监督学习在这种情况下的表现不佳。

强化学习的特点：
- 输入的样本是**序列**数据。
- 环境可能会告诉我们这个动作是错误的，但是它并没有告诉我们正确的动作是什么，只能通过**尝试**来学习。
- 试错探索`trial-and-error exploration`：探索`exploration`和利用`exploitation`是RL的核心问题。
  - 探索：尝试一些新的动作，可能会得到更多奖励，也有可能变得一无所有
  - 利用：采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。
  - 因此，我们需要在探索和利用之间进行权衡，这也是在监督学习里面没有的情况。
- 监督者`supervisor`只有奖励信号`reward signal`，而且奖励信号是延迟的。
- 智能体的动作会影响它随后得到的数据，如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕，怎么让智能体的动作一直稳定地提升是一个很大的挑战。
- 监督学习算法的上限`upper bound`就是人类的表现，标注结果决定了它的表现永远不可能超越人类。但是对于强化学习，它在环境里面自己探索，有非常大的潜力，它可以获得超越人类的能力的表现。

一场游戏称为一个**回合**`episode`(试验`trial`)，包括了开始到结束的状态、动作和奖励信息。
**轨迹**`trajectory`包含了从开始到结束的所有状态和动作，$\tau = (s_0, a_0, s_1, a_1, \cdots, s_T, a_T)$。其中$s_t$是时间步$t$的状态，$a_t$是时间步$t$的动作，$T$是轨迹的长度(可能是可变的)。
**预演**`rollout`从当前帧对动作进行采样，生成很多局游戏，$\text{rollout} = \{\tau_1, \tau_2, \cdots, \tau_N\}$。其中$\tau_i$是第$i$个轨迹，$N$是预演的次数。
